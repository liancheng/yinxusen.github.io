
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Crazy Small Files in HDFS - wtf AI ?</title>
  <meta name="author" content="Xusen">

  
  <meta name="description" content="key ideas: 癫狂小文件：由small files input探索HDFS：功用、功效、策略
Spark的处理方式：partition自己定制，把O(n2)的shuffle变成O(n)的？
Hbase作为替代
Hdfs combine file与multi file对比， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://yinxusen.github.io/blog/2014/03/11/crazy-small-files-in-hdfs">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="wtf AI ?" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">wtf AI ?</a></h1>
  
    <h2>Gee...  I don't know what AI means...</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:yinxusen.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/aboutme">About Me</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Crazy Small Files in HDFS</h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-03-11T16:28:47+08:00" pubdate data-updated="true">Mar 11<span>th</span>, 2014</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>key ideas:</p>

<ul>
<li><p>癫狂小文件：由small files input探索HDFS：功用、功效、策略</p></li>
<li><p>Spark的处理方式：partition自己定制，把O(n<sup>2</sup>)的shuffle变成O(n)的？</p></li>
<li><p>Hbase作为替代</p></li>
<li><p>Hdfs combine file与multi file对比，后者为什么倍deprecated</p></li>
<li><p>Shuffle为什么存在，如何避免，是在底层（HDFS）还是在上层（Spark应用）？</p></li>
<li><p>偶尔要为replication容错付出更多代价</p></li>
<li><p>Combine file input format，blog上错误多影响广，以及不友好的API，糟糕的示例和注释。</p></li>
<li><p>实际场景？学术研究VS线上应用。什么才是LDA输入的最佳实践？Mahout的处理方式？</p></li>
<li><p>文件系统的设计思考，可引入我自己的文件系统。</p></li>
<li><p>代码的简洁如何保证？同时如何保证性能？隐藏在简洁代码中的黑魔法。</p></li>
<li><p>Partition，split，以及种种locaility preserve的思考（为什么程序性能这么差？1024个partition从何而来？）</p></li>
<li><p>Google在bigtable和GFS的思考</p></li>
<li><p>LineRecorder中的readline函数如何实现越过block向后看？</p></li>
</ul>


<p><strong>LDA best practice:</strong></p>

<p>I think there are two common ways to use LDA in practice. First is the use in experimental condition, say, you have a bunch of small files on your disk. Then you want to upload them into HDFS, and call LDA in Spark. This is a usual way if you just want to do some experiments with LDA. In other words, it is an off-line training process. The second way of using LDA is an industrial use. You may have a streaming pipe, which will transport new feeds in Twitter or some other websites into your system. You will choose to put those feeds into a distributed storage such as HDFS or HBase, or you just send the streaming into a process.</p>

<p>Think them boldly, they are totally different. With respect to the usage of LDA, we should take care of the two scenarios simultaneously. Both of them are useful so we should not give up each of them.</p>

<p><strong>Offline scenario of LDA</strong></p>

<p>In the offline scenario, you may not charge of the pre-processing, instead you just leave it to the end-user. Users will change the raw texts into the format you want, and upload them into HDFS so your LDA application can read them directly. In this way, what we need to do is just specify the input format. What a relief !</p>

<p>Maybe you can help end-users one step more. You write a program, single or parallel, whatever, to help the pre-process for end-user. Just like what Mahout does. End-user should write a ugly shell program as coordinator, to control the overall workflow. In this way, you can write a program to change the small files (raw texts) into a huge file which lines represents texts, with filenames in the front of the line plus a separator.</p>

<p>But, I think a better way is melding the pre-process with LDA. What the end-user does is just upload his raw texts on HDFS. In this way, we must provide the function to read all texts and their corresponding filenames in. Then we implement a <code>CombineFileInputFormat</code>, a <code>CombineFileRecordReader</code>, a <code>FileLineWritable</code> and an interface looks like <code>textFiles</code> to support the scenario.</p>

<p>I am not mean that it&rsquo;s a best practice. Indeed, it is very bad to put lots of small files on HDFS, for it will occupy so many index entries than bad performance will occur. I just talk about one feasible way. However, there are some tangle problems we must solve.</p>

<p>First of all is the block size of your HDFS. Although we mean &ldquo;small files&rdquo;, but how small it is? Will its size larger than a single block in HDFS? The answer is Yes, it is possible. So we must handle the joint of blocks for each file, especially when the file is a multi-byte one, say, UTF encoded. Characters on the edge of blocks will be separated into two parts. We must take the responsibility to merge them together seamlessly.</p>

<p>The key point is, we do not like shuffle, especially the unnecessary one. We hope that blocks of each file could be stay in the same node, so that we can merge them together without shuffle. As with the <a href="http://www.idryman.org/blog/2013/09/22/process-small-files-on-hadoop-using-combinefileinputformat-1/">blog</a> says, if we override the <code>isSplitable()</code> function and set the return value to false, then we can keep a single file in the same <code>split</code>. <code>HadoopRDD</code> treats a <code>split</code> as a single partition. If so, we just need to merge blocks of a single file in one partition without any shuffle. Very happy!</p>

<p>However, we find that what the blog says is wrong. The <code>isSplitable()</code> is really useful, but just for <code>FileInputFormat</code>, which is the parent class of <code>CombineFileInputFormat</code>. The latter has a very complicated logic to, you know, divide blocks into splits. However, in order to improve the read performance, <code>CombineFileInputFormat</code> constructs a node list and a rack list, to chain blocks in the same node, or the same rack together, and send them into the same split. Remember that HDFS has replications. If there are 3 replications of a block, then <code>CombineFileInputFormat</code> could have the possibility to read any of the 3 replications, so we cannot ensure that the class will read any blocks from any nodes! It seems that a global shuffle is inevitable.</p>

<p>Funny, we also find that there is a class named <code>MultiFileInputFormat</code>, which is the predecessor of <code>CombineFileInputFormat</code>. It is deprecated now, because of the low efficiency due to unawareness of data locality (per node / rack).</p>

<p>There must be some trade-offs here. We should solve the shuffle in HDFS level, or we need solve the shuffle in Spark level. If sizes of all files are smaller than the block size, there is nothing hard to solve. But we cannot assume things like that.</p>

<p><strong>Online scenario of LDA</strong></p>

<p>Now let&rsquo;s turn into another direction. Note that a online product will never take the way described above. You know, people comes from data process division would not be silly to save all raw texts in local disks, then upload it to servers when processing. Massive data, in my opinion, should be stay in an appropriate place. In this scenario, raw texts or web page should be stored in a KV store, such as HBase. Small texts or articles should be treated same with pictures from websites. So, in reality, HBase will be used. However, Spark has no external storage except HDFS and local disk. So I think it is the time to add new storage.</p>

<p><strong>In case of using a our own customized partitioner</strong></p>

<p>Ah&hellip; It&rsquo;s awful! You know what, one of the reasons than Spark beats its counterparts is customized partitioners. It is the first time that you can arrange your items according to your wishes such easily. One idea to settle our problem is using a customized partitioner to rearrange our KV pairs. However, new partitioner is useful only when we do join of two RDDs iteratively, such as <code>PageRank</code>. But if we only need to shuffle things once time, it will be helplessness, for you cannot avoid the first-time shuffle.</p>

<p><strong>Where the shuffle from? How to do trade-off?</strong></p>

<p>We talked about needless shuffle just now. So question is, where is the shuffle from, and how to do trade-off? First, huge-files (or small files with smaller block size) could be cut off due to the fixed block size, so we want blocks belong to a single file could stay in the same split (partition, in dialect of Spark), then we can combine them together to recover the single file without any shuffle. The process is essential, because there will be multi-bytes characters such as UTF could be split.</p>

<p>Second, <code>CombineFileInputFormat</code> cannot preserve the property for us, due to the consideration of efficiency (see <code>CombineFileInputFormat</code> and <code>MultiFileInputFormat</code> as an example). Mostly because of the replication in HDFS, the fault tolerance function in HDFS, blocks of a single file could be read at any nodes.</p>

<p>So there are the trade-offs between &ldquo;shuffle HDFS level&rdquo; with &ldquo;shuffle Spark level&rdquo;, and between &ldquo;efficiency when reading blocks&rdquo; with &ldquo;efficiency due to shuffle-free&rdquo;, and eventually between &ldquo;efficiency&rdquo; with &ldquo;security&rdquo;.</p>

<p><strong>Rethink the design of file system, efficiency and robustness</strong></p>

<p><strong>Rethink the compact code and black magic behind it</strong></p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Xusen</span></span>

      








  


<time datetime="2014-03-11T16:28:47+08:00" pubdate data-updated="true">Mar 11<span>th</span>, 2014</time>
      


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://yinxusen.github.io/blog/2014/03/11/crazy-small-files-in-hdfs/" data-via="" data-counturl="http://yinxusen.github.io/blog/2014/03/11/crazy-small-files-in-hdfs/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2014/01/18/how-to-use-spark-for-ml-algorithms-and-why/" title="Previous Post: How to use Spark for ML algorithms and why ?">&laquo; How to use Spark for ML algorithms and why ?</a>
      
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>About Me</h1>
  <p>When machine learning meets system.</p>
  <p>新浪微博: <a href="http://weibo.com/yinxusen">@yinxusen</a><br/>
     LinkedIn: <a href="http://www.linkedin.com/in/xusenyin">Xusen Yin</a><br/>
     Github: <a href="https://github.com/yinxusen">@yinxusen</a>
  </p>
</section>
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/03/11/crazy-small-files-in-hdfs/">Crazy Small Files in HDFS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/01/18/how-to-use-spark-for-ml-algorithms-and-why/">How to Use Spark for ML Algorithms and Why ?</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/01/17/adl45-meeting-record/">ADL45 Meeting Record</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/01/17/petuum-source-code-read-and-initial-test-result/">Petuum: Source Code Read and Initial Test Result</a>
      </li>
    
  </ul>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - Xusen -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
